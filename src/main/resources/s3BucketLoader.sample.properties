#################################
# S3BucketLoader config file
#################################

# name prefix for the 'control channel' SNS topic
# that will be created to coordinate workers
aws.sns.control.topic.name=s3BucketLoaderControlChannel

# name of the SQS 'table of contents' queue
# that all workers consume from
aws.sqs.queue.name=s3BucketLoaderTOCQueue

# AWS creds to manage the above resources
# You will need to tweak this user's IAM 
# policies to permit appropriate SNS/SQS access
aws.access.key=YOUR_ACCESS_KEY
aws.secret.key=YOUR_SECRET_KEY
aws.account.principal.id=121212121221
aws.user.arn=arn:aws:iam::121212121221:user/your.s3bucketLoader.username


############################
# MASTER mode configs
# - will only be consumed
#   if -DisMaster=true is set
############################

# SourceTOCGenerator - the class used to generate
# the 'table of contents', the default is this DirectoryCrawler
# which will scan the 'source.dir' configured below
tocGenerator.class=org.bitsofinfo.s3.toc.DirectoryCrawler
tocGenerator.source.dir=/opt/nfs/toc_source

# total number of workers we expect to be running
# and consuming the toc tasks we create. The higher
# this number the faster it all works... you just
# need to provide and launch the workers... which
# if you turn 'master.workers.ec2.managed=true' on
# will be much more seamless (and cost you $$)
# NOTE: the master will not start publishing the TOC
# until this number of workers have reported as
# initialized. (if using ec2, the master has logic
# to detect and auto-terminate suspect workers that
# have yet to report in, to keep things moving
master.workers.total=4

# number of threads which consume
# the TOC entries the TOC generator creates and
# dispatch them to the SQS TOC queue. 
master.tocqueue.dispatch.threads=8

# Workers send period 'current' summary
# messages over the control channel which contain
# stats on the number of successes/fails for both
# WRITE and VALIDATE modes, if any of these 
# CURRENT_SUMMARY contain failures this setting
# controls if the master will stop the writes/validations
# currently in progress and immediately switch 
# to REPORT_ERRORS mode.... If this is false
# master will only go into ERROR_REPORT mode
# when workers are complete and send their FINISHED_SUMMARY
master.failfast.on.worker.current.summary.error=true

# OPTIONAL, this will use workers.total to spin up ec2 instances
# otherwise you are responsible for setting up workers
# and getting them ready. If you use this it can cost
# you $$ and you will want to use the userDataFile
# contents to automate the 'setup' of your worker nodes
master.workers.ec2.managed=false
master.workers.ec2.ami.id=ami-08842d60
master.workers.ec2.instanceType=t2.micro
master.workers.ec2.keyName=myKey
master.workers.ec2.securityGroupId=sg-3a8d065f
master.workers.ec2.subnetId=subnet-80d1f3a8
master.workers.ec2.shutdownBehavior=Terminate
master.workers.ec2.userDataFile=/path/to/ec2-init-s3BucketLoader.py


############################
# WORKER mode configs
# - will only be consumed
#   if -DisMaster=false is set
############################

# Total number of SQS TOC queue 
# consumer threads that will run
# on each worker node. You will want
# to tweak this based on the number of 
# cores your worker boxes have. Also
# consider that if you are ultimately writing
# through yas3fs, you have to account for 
# the threads yas3fs can potentially need
# as well. 
worker.toc.consumer.threads.num=4

# OPTIONAL: woker initialize command
# this will be run before the worker
# reports itself as INITIALIZED
# in this example we fire up yas3fs
# on the node to mount the target S3 
# bucket that the worker node(s) will 
# write to. This obviously assumes that your
# worker has the software required to run
# the comamnds below....@see 'master.workers.ec2.userDataFile'
worker.initialize.cmd=/path/to/yas3fs s3://BUCKET-NAME /opt/s3BucketLoader -l /path/to/yas3fs.log -d --st-blksize 131072 --read-retries-num 10 --read-retries-sleep 1 --download-retries-num 20 --download-retries-sleep 5 --recheck-s3 --cache-path /path/to/yas3fs/cache/s3BucketLoader --cache-on-disk 0 --cache-disk-size 30000 --with-plugin-class RecoverYas3fsPlugin --aws-managed-encryption --log-backup-count 20 --log-backup-gzip --log-mb-size 100
worker.initialize.cmd.env=AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY,AWS_SECRET_ACCESS_KEY=YOUR_SECRET_KEY
worker.destroy.cmd=fusermount -u /opt/s3BucketLoader

# OPTIONAL: a 'writeMonitor" which monitors 
# yas3fs to determine when it is really complete
# with all background uploads (checks s3_queue status
# for N number of times for it being consistently zero (0)
# All rsyncs can finish, yet yas3fs could be uploading in 
# the background...
worker.write.complete.monitor.class=org.bitsofinfo.s3.yas3fs.Yas3fsS3UploadMonitor
worker.write.complete.monitor.yas3fs.checkEveryMS=30000
worker.write.complete.monitor.yas3fs.logFilePath=/path/to/yas3fs.log

# TOCPayloadHandler - there are two TOC 
# payload handlers for both modes (WRITE/VALIDATE)
# For each consumed TOC message, these are invoked
# depending on the mode (i.e. rsyncing from source.dir -> target.dir)
# or checking for file existance and size during validate
tocPayloadHandler.write.class=org.bitsofinfo.s3.toc.RSyncInvokingTOCPayloadHandler
tocPayloadHandler.validate.class=org.bitsofinfo.s3.toc.ValidatingTOCPayloadHandler

# This dir should have access to the shared copy of 
# source data that the TOC was generated from
tocPayloadHandler.source.dir.root=/opt/nfs/toc_source

# This dir should be the 'target' dir of where
# the files will be copied to (i.e. this would be the yas3fs s3 mount root)
tocPayloadHandler.target.dir.root=/opt/s3BucketLoader


 
