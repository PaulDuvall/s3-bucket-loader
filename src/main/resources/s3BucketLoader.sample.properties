#################################
# S3BucketLoader config file
#################################

# name prefix for the 'control channel' SNS topic
# that will be created to coordinate workers
aws.sns.control.topic.name=s3BucketLoaderControlChannel

# name of the SQS 'table of contents' queue
# that all workers consume from
aws.sqs.queue.name=s3BucketLoaderTOCQueue

# AWS creds to manage the above resources
# You will need to tweak this user's IAM 
# policies to permit appropriate SNS/SQS access
aws.access.key=YOUR_ACCESS_KEY
aws.secret.key=YOUR_SECRET_KEY
aws.account.principal.id=121212121221
aws.user.arn=arn:aws:iam::121212121221:user/your.s3bucketLoader.username


############################
# MASTER mode configs
# - will only be consumed
#   if -DisMaster=true is set
############################


# OPTIONAL: if the master should auto-shutdown itself
# after N minutes after the entire process is complete
# (equivlent to you manually doing a ^C on the app
master.auto.shutdown.after.n.minutes=30

# OPTIONAL: If you want the master to control (start | stop) 
# an ec2 instance identified by 'instanceId' which contains
# the 'source' data that workers/master will access for TOC ops
master.source.host.ec2.instanceId=i-xxxxxxx
master.source.host.ec2.stopOnMasterShutdown=false
master.source.host.post.start.cmd=mount 1.2.3.4:/exported/nfs /opt/nfs/toc_source
master.source.host.pre.stop.cmd=umount /opt/nfs/toc_source

# SourceTOCGenerator - the class used to generate
# the 'table of contents', there are several different
# ones you can use

# SourceTOCGenerator: default => DirectoryCrawler
# which will scan the 'source.dir' configured below
tocGenerator.class=org.bitsofinfo.s3.toc.DirectoryCrawler
tocGenerator.source.dir=/opt/nfs/toc_source

# OPTIONAL: if set, will skip all files who's modified at
# timestamp is OLDER than this date...
tocGenerator.lastModifiedAtGreaterThanFilter=2014-10-22

# SourceTOCGenerator: TOCFileReaderGenerator
# which will scan the 'source.dir' configured below
# using the manifest file which lists directory paths
# of files that should be sent to the TOC queue
# where the paths in the manifest are /relative/from/toc_source_dir
tocGenerator.class=org.bitsofinfo.s3.toc.TOCManifestBasedGenerator
tocGenerator.toc.manifest.file=/opt/nfs/toc_manifest.txt
tocGenerator.source.dir=/opt/nfs/toc_source

# SourceTOCGenerator: S3BucketObjectLister
# which will scan the S3 'bucketName' configured below
# to generate a TOC of paths relative/from/bucket-root
tocGenerator.class=org.bitsofinfo.s3.toc.S3BucketObjectLister
tocGenerator.source.s3.bucketName=source-bucket-name


# total number of workers we expect to be running
# and consuming the toc tasks we create. The higher
# this number the faster it all works... you just
# need to provide and launch the workers... which
# if you turn 'master.workers.ec2.managed=true' on
# will be much more seamless (and cost you $$)
# NOTE: the master will not start publishing the TOC
# until this number of workers have reported as
# initialized. (if using ec2, the master has logic
# to detect and auto-terminate suspect workers that
# have yet to report in, to keep things moving
master.workers.total=4

# If configured, on SHUTDOWN, the master
# will instruct all workers to upload the
# named files to the given bucket at a location
# as follows
#
#  Workers: BUCKET_NAME/[master-generated-name]/[workerIP]/
#  Master: BUCKET_NAME/[master-generated-name]/master/
master.s3.log.bucket=s3-bkt-ldr-logs
master.s3.log.worker.files=/opt/s3BucketLoader/s3BucketLoader.log, /var/log/yas3fs/yas3fs.log
master.s3.log.master.files=/opt/s3BucketLoader/s3BucketLoader.log, /some/dir/worker_error_reports.json

# If ERROR report mode is triggered
# specify the full path to the logfile
# where the JSON of error reports will be logged
master.workers.error.report.logfile=/some/dir/worker_error_reports.json

# number of threads which consume
# the TOC entries the TOC generator creates and
# dispatch them to the SQS TOC queue. 
master.tocqueue.dispatch.threads=8

# Workers send period 'current' summary
# messages over the control channel which contain
# stats on the number of successes/fails for both
# WRITE and VALIDATE modes, if any of these 
# CURRENT_SUMMARY contain failures this setting
# controls if the master will stop the writes/validations
# currently in progress and immediately switch 
# to REPORT_ERRORS mode.... If this is false
# master will only go into ERROR_REPORT mode
# when workers are complete and send their FINISHED_SUMMARY
master.failfast.on.worker.current.summary.error=true

# OPTIONAL, this will use workers.total to spin up ec2 instances
# otherwise you are responsible for setting up workers
# and getting them ready. If you use this it can cost
# you $$ and you will want to use the userDataFile
# contents to automate the 'setup' of your worker nodes
master.workers.ec2.managed=false
master.workers.ec2.minutes.to.wait.for.worker.init=10
master.workers.ec2.ami.id=ami-08842d60
master.workers.ec2.instanceType=t2.micro
master.workers.ec2.disk.deviceName=/dev/xvda
master.workers.ec2.disk.volumeType=Standard
master.workers.ec2.disk.size.gigabytes=30
master.workers.ec2.keyName=myKey
master.workers.ec2.securityGroupId=sg-3a8d065f
master.workers.ec2.subnetId=subnet-80d1f3a8
master.workers.ec2.shutdownBehavior=Terminate
master.workers.ec2.userDataFile=/path/to/ec2-init-s3BucketLoader.py


############################
# WORKER mode configs
# - will only be consumed
#   if -DisMaster=false is set
############################

# Total number of SQS TOC queue 
# consumer threads that will run
# on each worker node. You will want
# to tweak this based on the number of 
# cores your worker boxes have. Also
# consider that if you are ultimately writing
# through yas3fs, you have to account for 
# the threads yas3fs can potentially need
# as well. 
worker.toc.consumer.threads.num=4

# number of TOC message requests a worker must make before
# it will be a candidate for idle evaluation
worker.toc.consumer.threads.min.requests.before.idle=60

# OPTIONAL: woker initialize command
# this will be run before the worker
# reports itself as INITIALIZED
# in this example we fire up yas3fs
# on the node to mount the target S3 
# bucket that the worker node(s) will 
# write to. This obviously assumes that your
# worker has the software required to run
# the commands below....@see 'master.workers.ec2.userDataFile'
worker.initialize.cmd=/path/to/yas3fs s3://BUCKET-NAME /opt/s3BucketLoader -l /path/to/yas3fs.log -d --st-blksize 131072 --read-retries-num 10 --read-retries-sleep 1 --download-retries-num 20 --download-retries-sleep 5 --recheck-s3 --cache-path /path/to/yas3fs/cache/s3BucketLoader --cache-on-disk 0 --cache-disk-size 30000 --with-plugin-class RecoverYas3fsPlugin --aws-managed-encryption --log-backup-count 20 --log-backup-gzip --log-mb-size 100
worker.initialize.cmd.env=AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY,AWS_SECRET_ACCESS_KEY=YOUR_SECRET_KEY
worker.destroy.cmd=fusermount -u /opt/s3BucketLoader


# OPTIONAL: worker pre-VALIDATE mode commands
# delimiter for each command = ;
# These will be run as soon as VALIDATE mode is started
worker.pre.validate.cmd.env=%worker.initialize.cmd.env%
worker.pre.validate.cmd=fusermount -u /opt/s3BucketLoader; rm -rf /yas3fs/cache/s3BucketLoader/*; %worker.initialize.cmd%

# OPTIONAL: a 'writeBackoffMonitor" which monitors 
# yas3fs to determine when to backoff based on
# checking the s3_queue number, if it gets to a certain
# point all TOCQueue consuming threads will pause until
# s3_queue drops (yas3fs writes in background and we don't
# want to overload it). This also has the option via the 
# backoffWhenTotalHTTPSConns to backoff when the total #
# of outgoing HTTPS conns is >= the configured number
# Additionally it can backoff of the total yas3fs
# multipart-uploads are >= N 
#
# IMPORTANT the value for backoffWhenTotalHTTPSConns should take
# into account the worker.toc.consumer.threads.num value and
# factor that into the potential number.
worker.write.backoff.monitor.class=org.bitsofinfo.s3.yas3fs.Yas3fsS3UploadMonitor
worker.write.backoff.monitor.yas3fs.backoffWhenMultipartUploads=2
worker.write.backoff.monitor.yas3fs.backoffWhenTotalHTTPSConns=20
worker.write.backoff.monitor.yas3fs.backoffWhenTotalS3Uploads=10
worker.write.backoff.monitor.yas3fs.checkEveryMS=30000
worker.write.backoff.monitor.yas3fs.logFilePath=/path/to/yas3fs.log

# OPTIONAL: a 'writeErrorMonitor" which monitors 
# yas3fs log file for ERROR entries which occur in the background
# asynchronously and will represent errors uploading to S3, AFTER
# a file may have 'successfully' been written locally by the 
# 'tocPayloadHandler.write.class' below...
worker.write.error.monitor.class=org.bitsofinfo.s3.yas3fs.Yas3fsS3UploadMonitor
worker.write.error.monitor.yas3fs.checkEveryMS=30000
worker.write.error.monitor.yas3fs.logFilePath=/path/to/yas3fs.log

# OPTIONAL: a 'writeMonitor" which monitors 
# yas3fs to determine when it is really complete
# with all background uploads (checks s3_queue status
# for N number of times for it being consistently zero (0)
# All rsyncs can finish, yet yas3fs could be uploading in 
# the background...
worker.write.complete.monitor.class=org.bitsofinfo.s3.yas3fs.Yas3fsS3UploadMonitor
worker.write.complete.monitor.yas3fs.checkEveryMS=30000
worker.write.complete.monitor.yas3fs.logFilePath=/path/to/yas3fs.log

# TOCPayloadHandler - there are two TOC 
# payload handlers for both modes (WRITE/VALIDATE)
# For each consumed TOC message, these are invoked
# depending on the mode (i.e. copying from source.dir -> target.dir)
# or checking for file existance and size during validate
# FileCopyTOCPayloadHandler does (mkdir -p + (rsync | cp) + (optional chown/chmod))
tocPayloadHandler.write.class=org.bitsofinfo.s3.toc.FileCopyTOCPayloadHandler

# number of retries for each FileCopy operation (mkdir, rsync | cp) etc
tocPayloadHandler.write.retries=3
tocPayloadHandler.write.retries.sleep.ms=5000

# if set to FALSE, will just exec a standard "cp"
tocPayloadHandler.write.use.rsync=true

# rsync options, note this will be split on spaces
# when its time to parse it
tocPayloadHandler.write.rsync.options=--inplace -avz

# OPTIONAL regex for permissible rsync errors that will be ignored and not reported
# for example this this case the TOC source where paths are generated may be newer
# than the copy the workers have, so we don't expect them all to work.
tocPayloadHandler.write.rsync.tolerable.error.regex=(?s)(.*change_dir.*\\/opt\\/nfs.*No such file or directory.*)

# OPTIONAL: if this property is set, the FileCopyTOCPayloadHandler 
# (immeditaly a successful local write to the target dir) 
# will attempt to validate the file exists locally and its size matches
# up w/ what the TOC states by checking locally at the given path below
# the result of this check will be written to the specified log file
# NOTE the success of failure of this particular 'pre-validate' check
# has no effect on the overall success/failure of the WRITE operation
# NOTE: the log file will only contain validation FAILURES...
tocPayloadHandler.write.post.success.validate.local.dir=/path/to/yas3fs/cache/root/files
tocPayloadHandler.write.post.success.validate.logfile=/opt/s3BucketLoader/yas3fs-post-write-cache-validate.log
tocPayloadHandler.write.post.success.validate.skipDirectories=true

# OPTIONAL for FileCopyTOCPayloadHandler
# these will be executed after the copy
# of each file path, and configurable if to
# be applied to directories ONLY or both files/dirs
tocPayloadHandler.write.chmod.dirsOnly=true
tocPayloadHandler.write.chmod=775 
tocPayloadHandler.write.chown.dirsOnly=true
tocPayloadHandler.write.chown=500:500

# for VALIDATE mode, this one by default checks file existence AND size match up
tocPayloadHandler.validate.class=org.bitsofinfo.s3.toc.ValidatingTOCPayloadHandler

# for VALIDATE mode
# - validateEverywhere = will check both local FS + S3 object meta-data
# - validateLocallyOnly = will check ONLY local FS 
# - validateS3Only = will check ONLY s3
# - validateLocallyThenS3OnFailure = will check local FS, if that fails, then check on S3
tocPayloadHandler.validate.mode=validateLocallyThenS3OnFailure
tocPayloadHandler.validate.s3.bucketName=bucketNameToValidateIfS3ModeEnabled

# This dir should have access to the shared copy of 
# source data that the TOC was generated from
tocPayloadHandler.source.dir.root=/opt/nfs/toc_source

# This dir should be the 'target' dir of where
# the files will be copied to (i.e. this would be the yas3fs s3 mount root)
tocPayloadHandler.target.dir.root=/opt/s3BucketLoader


# TOCPayloadHandler - S3KeyCopyingTOCPayloadHandler
# which executes S3 key copies for every TOC entry received
tocPayloadHandler.write.class=org.bitsofinfo.s3.toc.S3KeyCopyingTOCPayloadHandler
tocPayloadHandler.write.s3keyCopy.sourceS3BucketName=s3-bkt-ldr-repo
tocPayloadHandler.write.s3keyCopy.targetS3BucketName=yas3fs-liferay-prod-001-backup
tocPayloadHandler.write.s3keyCopy.storageClass=ReducedRedundancy
tocPayloadHandler.write.s3keyCopy.enableServerSideEncryption=true

 
